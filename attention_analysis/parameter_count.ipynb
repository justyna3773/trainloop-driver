{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notatnik do weryfikacji liczby parametrów danych architektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from automatic_feature_selector import *\n",
    "from utils import FEATURE_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH=r'C:\\initial_model\\historic_synthetic_dnnevo\\RANDOM\\FINAL_MODELS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "def params_analysis(model):\n",
    "    def count_params(module):\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "    policy = model.policy\n",
    "    print(\"=== TOTAL POLICY PARAMS ===\")\n",
    "    total_policy_params = count_params(policy)\n",
    "    print(f\"Total policy params: {total_policy_params}\\n\")\n",
    "\n",
    "    # Keep track of which parameter names we've already counted\n",
    "    counted_names = set()\n",
    "\n",
    "\n",
    "    def add_group(name, module):\n",
    "        \"\"\"Print param count for a module and mark its params as counted.\"\"\"\n",
    "        if module is None:\n",
    "            print(f\"{name}: <None>\")\n",
    "            return\n",
    "        n = count_params(module)\n",
    "        print(f\"{name:20s}: {n:8d} params\")\n",
    "        for pname, _ in module.named_parameters():\n",
    "            counted_names.add(pname)\n",
    "\n",
    "\n",
    "    print(\"=== MAIN GROUPS ===\")\n",
    "    # 1) Feature extractor\n",
    "    add_group(\"features_extractor\", policy.features_extractor)\n",
    "\n",
    "    # 2) LSTMs (actor / critic / shared)\n",
    "    if hasattr(policy, \"lstm_actor\") and policy.lstm_actor is not None:\n",
    "        add_group(\"lstm_actor\", policy.lstm_actor)\n",
    "    if hasattr(policy, \"lstm_critic\") and policy.lstm_critic is not None:\n",
    "        add_group(\"lstm_critic\", policy.lstm_critic)\n",
    "    if hasattr(policy, \"lstm\") and policy.lstm is not None:\n",
    "        add_group(\"lstm_shared\", policy.lstm)\n",
    "\n",
    "    # 3) MLP extractor (policy + value nets inside)\n",
    "    add_group(\"mlp_extractor\", policy.mlp_extractor)\n",
    "\n",
    "    # 4) Heads: action + value\n",
    "    add_group(\"action_net\", policy.action_net)\n",
    "    add_group(\"value_net\", policy.value_net)\n",
    "\n",
    "    # 5) Distribution-specific params (e.g., log_std for continuous actions)\n",
    "    if hasattr(policy.action_dist, \"log_std\") and isinstance(\n",
    "        getattr(policy.action_dist, \"log_std\", None), th.nn.Parameter\n",
    "    ):\n",
    "        print(\"=== ACTION DIST PARAMS ===\")\n",
    "        print(f\"log_std: {policy.action_dist.log_std.numel():8d} params\")\n",
    "        counted_names.add(\"action_dist.log_std\")\n",
    "\n",
    "\n",
    "    print(\"\\n=== BREAKDOWN INSIDE MLP EXTRACTOR ===\")\n",
    "    # Optional: split MLP extractor into policy and value parts\n",
    "    if hasattr(policy.mlp_extractor, \"policy_net\"):\n",
    "        add_group(\"mlp_policy_net\", policy.mlp_extractor.policy_net)\n",
    "    if hasattr(policy.mlp_extractor, \"value_net\"):\n",
    "        add_group(\"mlp_value_net\", policy.mlp_extractor.value_net)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # NEW: print feature extractor configuration\n",
    "    # ---------------------------------------------------------------------\n",
    "    fe = policy.features_extractor\n",
    "\n",
    "    print(\"\\n=== FEATURE EXTRACTOR CONFIG ===\")\n",
    "    print(f\"Class: {fe.__class__.__name__}\")\n",
    "    print(\"\\nPyTorch module repr:\\n\")\n",
    "    print(fe)  # this prints the full nn.Module structure\n",
    "\n",
    "    print(\"\\nNon-module attributes (likely hyperparameters):\")\n",
    "    for attr, val in fe.__dict__.items():\n",
    "        # skip internal / framework stuff\n",
    "        if attr.startswith(\"_\") or attr in (\"training\",):\n",
    "            continue\n",
    "        # skip submodules and parameters to avoid huge prints\n",
    "        if isinstance(val, (th.nn.Module, th.nn.Parameter)):\n",
    "            continue\n",
    "        print(f\"  {attr}: {val!r}\")\n",
    "\n",
    "\n",
    "    print(\"\\n=== UNACCOUNTED POLICY PARAMS (by name) ===\")\n",
    "    # Find any params we didn't assign to a group\n",
    "    unaccounted = []\n",
    "    for pname, p in policy.named_parameters():\n",
    "        if pname not in counted_names:\n",
    "            unaccounted.append((pname, p.numel()))\n",
    "\n",
    "    if not unaccounted:\n",
    "        print(\"All parameters accounted for in the groups above.\")\n",
    "    else:\n",
    "        for pname, numel in unaccounted:\n",
    "            print(f\"{pname:40s}: {numel:8d} params\")\n",
    "\n",
    "    print(\"\\n=== CHECK SUM ===\")\n",
    "    group_sum = 0\n",
    "    for pname, p in policy.named_parameters():\n",
    "        if pname in counted_names:\n",
    "            group_sum += p.numel()\n",
    "    group_sum += sum(numel for _, numel in unaccounted)\n",
    "    print(f\"Sum of groups + leftovers: {group_sum}\")\n",
    "    print(f\"Total policy params       : {total_policy_params}\")\n",
    "\n",
    "\n",
    "    # Raw policy_kwargs as stored by the algorithm\n",
    "    print(\"policy_kwargs from model:\")\n",
    "    print(model.policy_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "Model 1 - model z wszystkimi metrykami wejściowymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_PATH =r'.\\FINAL_MODELS\\ATTENTION\\attention_MlpLstmPolicy_mlplstm_attention_contrib_metric.zip'\n",
    "MODEL_PATH=r'C:\\Users\\ultramarine\\Desktop\\ppo_magisterka\\trainloop_driver_official\\trainloop_driver_final\\trainloop-driver\\FINAL_MODELS\\BASELINE\\baseline_3\\recurrentppo_MlpLstmPolicy_mlplstm_baseline_70_000_return_64_16.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "model = RecurrentPPO.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOTAL POLICY PARAMS ===\n",
      "Total policy params: 40136\n",
      "\n",
      "=== MAIN GROUPS ===\n",
      "features_extractor  :        0 params\n",
      "lstm_actor          :    18688 params\n",
      "lstm_critic         :    18688 params\n",
      "mlp_extractor       :     2624 params\n",
      "action_net          :      119 params\n",
      "value_net           :       17 params\n",
      "\n",
      "=== BREAKDOWN INSIDE MLP EXTRACTOR ===\n",
      "mlp_policy_net      :     1312 params\n",
      "mlp_value_net       :     1312 params\n",
      "\n",
      "=== FEATURE EXTRACTOR CONFIG ===\n",
      "Class: FlattenExtractor\n",
      "\n",
      "PyTorch module repr:\n",
      "\n",
      "FlattenExtractor(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "\n",
      "Non-module attributes (likely hyperparameters):\n",
      "\n",
      "=== UNACCOUNTED POLICY PARAMS (by name) ===\n",
      "mlp_extractor.policy_net.0.weight       :     1024 params\n",
      "mlp_extractor.policy_net.0.bias         :       16 params\n",
      "mlp_extractor.policy_net.2.weight       :      256 params\n",
      "mlp_extractor.policy_net.2.bias         :       16 params\n",
      "mlp_extractor.value_net.0.weight        :     1024 params\n",
      "mlp_extractor.value_net.0.bias          :       16 params\n",
      "mlp_extractor.value_net.2.weight        :      256 params\n",
      "mlp_extractor.value_net.2.bias          :       16 params\n",
      "action_net.weight                       :      112 params\n",
      "action_net.bias                         :        7 params\n",
      "value_net.weight                        :       16 params\n",
      "value_net.bias                          :        1 params\n",
      "lstm_actor.weight_ih_l0                 :     1792 params\n",
      "lstm_actor.weight_hh_l0                 :    16384 params\n",
      "lstm_actor.bias_ih_l0                   :      256 params\n",
      "lstm_actor.bias_hh_l0                   :      256 params\n",
      "lstm_critic.weight_ih_l0                :     1792 params\n",
      "lstm_critic.weight_hh_l0                :    16384 params\n",
      "lstm_critic.bias_ih_l0                  :      256 params\n",
      "lstm_critic.bias_hh_l0                  :      256 params\n",
      "\n",
      "=== CHECK SUM ===\n",
      "Sum of groups + leftovers: 40136\n",
      "Total policy params       : 40136\n",
      "policy_kwargs from model:\n",
      "{'lstm_hidden_size': 64, 'net_arch': [{'pi': [16, 16], 'vf': [16, 16]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'ortho_init': True}\n"
     ]
    }
   ],
   "source": [
    "params_analysis(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 - model z 3-ema metrykami wejściowymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_2=r'c:\\initial_model\\recurrentppo\\MlpLstmPolicy\\recurrentppo_MlpLstmPolicy_mlplstm.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "model = RecurrentPPO.load(MODEL_PATH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOTAL POLICY PARAMS ===\n",
      "Total policy params: 38088\n",
      "\n",
      "=== MAIN GROUPS ===\n",
      "features_extractor  :        0 params\n",
      "lstm_actor          :    17664 params\n",
      "lstm_critic         :    17664 params\n",
      "mlp_extractor       :     2624 params\n",
      "action_net          :      119 params\n",
      "value_net           :       17 params\n",
      "\n",
      "=== BREAKDOWN INSIDE MLP EXTRACTOR ===\n",
      "mlp_policy_net      :     1312 params\n",
      "mlp_value_net       :     1312 params\n",
      "\n",
      "=== FEATURE EXTRACTOR CONFIG ===\n",
      "Class: FlattenExtractor\n",
      "\n",
      "PyTorch module repr:\n",
      "\n",
      "FlattenExtractor(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "\n",
      "Non-module attributes (likely hyperparameters):\n",
      "\n",
      "=== UNACCOUNTED POLICY PARAMS (by name) ===\n",
      "mlp_extractor.policy_net.0.weight       :     1024 params\n",
      "mlp_extractor.policy_net.0.bias         :       16 params\n",
      "mlp_extractor.policy_net.2.weight       :      256 params\n",
      "mlp_extractor.policy_net.2.bias         :       16 params\n",
      "mlp_extractor.value_net.0.weight        :     1024 params\n",
      "mlp_extractor.value_net.0.bias          :       16 params\n",
      "mlp_extractor.value_net.2.weight        :      256 params\n",
      "mlp_extractor.value_net.2.bias          :       16 params\n",
      "action_net.weight                       :      112 params\n",
      "action_net.bias                         :        7 params\n",
      "value_net.weight                        :       16 params\n",
      "value_net.bias                          :        1 params\n",
      "lstm_actor.weight_ih_l0                 :      768 params\n",
      "lstm_actor.weight_hh_l0                 :    16384 params\n",
      "lstm_actor.bias_ih_l0                   :      256 params\n",
      "lstm_actor.bias_hh_l0                   :      256 params\n",
      "lstm_critic.weight_ih_l0                :      768 params\n",
      "lstm_critic.weight_hh_l0                :    16384 params\n",
      "lstm_critic.bias_ih_l0                  :      256 params\n",
      "lstm_critic.bias_hh_l0                  :      256 params\n",
      "\n",
      "=== CHECK SUM ===\n",
      "Sum of groups + leftovers: 38088\n",
      "Total policy params       : 38088\n",
      "policy_kwargs from model:\n",
      "{'lstm_hidden_size': 64, 'net_arch': [{'pi': [16, 16], 'vf': [16, 16]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'ortho_init': True}\n"
     ]
    }
   ],
   "source": [
    "params_analysis(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liczba parametrów architektury Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_PATH_ATT=rf'{BASE_PATH}\\ATTENTION\\attention_MlpLstmPolicy_mlplstm_att_tunedtolstm.zip'\n",
    "MODEL_PATH_ATT=r'c:\\initial_model\\recurrentppo\\MlpLstmPolicy\\recurrentppo_MlpLstmPolicy_mlplstm_attention_3_1.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "model = RecurrentPPO.load(MODEL_PATH_ATT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOTAL POLICY PARAMS ===\n",
      "Total policy params: 40376\n",
      "\n",
      "=== MAIN GROUPS ===\n",
      "features_extractor  :      240 params\n",
      "lstm_actor          :    18688 params\n",
      "lstm_critic         :    18688 params\n",
      "mlp_extractor       :     2624 params\n",
      "action_net          :      119 params\n",
      "value_net           :       17 params\n",
      "\n",
      "=== BREAKDOWN INSIDE MLP EXTRACTOR ===\n",
      "mlp_policy_net      :     1312 params\n",
      "mlp_value_net       :     1312 params\n",
      "\n",
      "=== FEATURE EXTRACTOR CONFIG ===\n",
      "Class: AttentionExtractor\n",
      "\n",
      "PyTorch module repr:\n",
      "\n",
      "AttentionExtractor(\n",
      "  (value_embed): Linear(in_features=1, out_features=7, bias=True)\n",
      "  (ln_e): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
      "  (Wq): Linear(in_features=7, out_features=8, bias=False)\n",
      "  (Wk): Linear(in_features=7, out_features=8, bias=False)\n",
      "  (Wv): Linear(in_features=7, out_features=7, bias=False)\n",
      "  (out_act): Tanh()\n",
      ")\n",
      "\n",
      "Non-module attributes (likely hyperparameters):\n",
      "  n_metrics: 7\n",
      "  d_embed: 7\n",
      "  d_k: 4\n",
      "  n_heads: 2\n",
      "  head_agg: 'mean'\n",
      "  mode: 'content_only'\n",
      "  attn_norm: 'row_softmax'\n",
      "  attn_temp: 1.2\n",
      "  use_posenc: True\n",
      "  use_residual: False\n",
      "  residual_weight: 1.0\n",
      "  use_content_embed: False\n",
      "  embedders: None\n",
      "  out_layernorm: False\n",
      "  out_activation: 'tanh'\n",
      "  post_proj: None\n",
      "  out_ln: None\n",
      "  attn_matrix: None\n",
      "  metric_importance: None\n",
      "  contrib_importance: None\n",
      "  attn_history: deque([], maxlen=1000)\n",
      "  contrib_history: deque([], maxlen=1000)\n",
      "  total_steps: 0\n",
      "\n",
      "=== UNACCOUNTED POLICY PARAMS (by name) ===\n",
      "features_extractor.P_idx                :       49 params\n",
      "features_extractor.head_logits          :        2 params\n",
      "features_extractor.value_embed.weight   :        7 params\n",
      "features_extractor.value_embed.bias     :        7 params\n",
      "features_extractor.ln_e.weight          :        7 params\n",
      "features_extractor.ln_e.bias            :        7 params\n",
      "features_extractor.Wq.weight            :       56 params\n",
      "features_extractor.Wk.weight            :       56 params\n",
      "features_extractor.Wv.weight            :       49 params\n",
      "mlp_extractor.policy_net.0.weight       :     1024 params\n",
      "mlp_extractor.policy_net.0.bias         :       16 params\n",
      "mlp_extractor.policy_net.2.weight       :      256 params\n",
      "mlp_extractor.policy_net.2.bias         :       16 params\n",
      "mlp_extractor.value_net.0.weight        :     1024 params\n",
      "mlp_extractor.value_net.0.bias          :       16 params\n",
      "mlp_extractor.value_net.2.weight        :      256 params\n",
      "mlp_extractor.value_net.2.bias          :       16 params\n",
      "action_net.weight                       :      112 params\n",
      "action_net.bias                         :        7 params\n",
      "value_net.weight                        :       16 params\n",
      "value_net.bias                          :        1 params\n",
      "lstm_actor.weight_ih_l0                 :     1792 params\n",
      "lstm_actor.weight_hh_l0                 :    16384 params\n",
      "lstm_actor.bias_ih_l0                   :      256 params\n",
      "lstm_actor.bias_hh_l0                   :      256 params\n",
      "lstm_critic.weight_ih_l0                :     1792 params\n",
      "lstm_critic.weight_hh_l0                :    16384 params\n",
      "lstm_critic.bias_ih_l0                  :      256 params\n",
      "lstm_critic.bias_hh_l0                  :      256 params\n",
      "\n",
      "=== CHECK SUM ===\n",
      "Sum of groups + leftovers: 40376\n",
      "Total policy params       : 40376\n",
      "policy_kwargs from model:\n",
      "{'features_extractor_class': <class 'automatic_feature_selector.attention_3_1.AttentionExtractor'>, 'features_extractor_kwargs': {'d_embed': 7, 'd_k': 4, 'n_heads': 2, 'head_agg': 'mean', 'use_posenc': True, 'alpha_mode': 'global', 'alpha_init': 0.5, 'learn_alpha': False, 'use_content_embed': False, 'attn_temp': 1.2, 'final_out_dim': 7, 'out_layernorm': False, 'out_activation': 'tanh', 'use_residual': False}, 'lstm_hidden_size': 64, 'n_lstm_layers': 1, 'net_arch': [{'pi': [16, 16], 'vf': [16, 16]}], 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'ortho_init': True, 'normalize_images': False}\n"
     ]
    }
   ],
   "source": [
    "params_analysis(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainloop_py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
