{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_1 ='../FINAL_MODELS/BASELINE/baseline_3/recurrentppo_MlpLstmPolicy_mlplstm_baseline_70_000_return_64_16.zip'\n",
    "MODEL_PATH_2='../FINAL_MODELS/POROWNANIE_PARAMETROW/recurrentppo_MlpLstmPolicy_mlplstm_experiment_2.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "model1 = RecurrentPPO.load(MODEL_PATH_1)\n",
    "model2 = RecurrentPPO.load(MODEL_PATH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch as th\n",
    "from sb3_contrib.common.recurrent.type_aliases import RNNStates\n",
    "\n",
    "\n",
    "def measure_model_compute_time(model, obs_dim=7, BATCH=256, STEPS=1000):\n",
    "    policy = model.policy\n",
    "\n",
    "    # Fake batch of observations; adjust obs_dim to your env\n",
    "    x = th.randn(BATCH, obs_dim, device=policy.device, dtype=th.float32)\n",
    "\n",
    "    # ---- LSTM state init for RecurrentActorCriticPolicy ----\n",
    "    # lstm_hidden_state_shape is (n_lstm_layers, n_seq, lstm_hidden_size)\n",
    "    n_layers, _, hidden_size = policy.lstm_hidden_state_shape\n",
    "\n",
    "    # Here we use n_seq = BATCH (one LSTM state per \"env\" in the batch)\n",
    "    h0 = th.zeros((n_layers, BATCH, hidden_size), device=policy.device)\n",
    "    c0 = th.zeros((n_layers, BATCH, hidden_size), device=policy.device)\n",
    "\n",
    "    # RNNStates has separate states for actor (pi) and critic (vf)\n",
    "    base_lstm_states = RNNStates(\n",
    "        pi=(h0, c0),\n",
    "        vf=(h0.clone(), c0.clone()),\n",
    "    )\n",
    "\n",
    "    # Episode starts flags: 1.0 = reset, 0.0 = continuation\n",
    "    # Shape (BATCH,)\n",
    "    episode_starts = th.zeros(BATCH, device=policy.device, dtype=th.float32)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(STEPS):\n",
    "        policy.optimizer.zero_grad()\n",
    "\n",
    "        # Re-init LSTM states each iteration so graphs don't chain forever\n",
    "        lstm_states = base_lstm_states\n",
    "\n",
    "        # RecurrentActorCriticPolicy.forward returns:\n",
    "        # actions, values, log_prob, new_lstm_states\n",
    "        actions, values, log_prob, lstm_states = policy.forward(\n",
    "            x, lstm_states, episode_starts\n",
    "        )\n",
    "\n",
    "        # Use differentiable outputs (values, log_prob) for a dummy loss\n",
    "        # Both are floating point with grad.\n",
    "        loss = values.mean() + log_prob.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        policy.optimizer.step()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    elapsed = t1 - t0\n",
    "    print(f\"ML compute time: {elapsed:.3f}s\")\n",
    "    return elapsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MEASURING OF JUST NETWORK COST (makes sense as stepping in the env takes time)\n",
    "import torch as th\n",
    "import time\n",
    "\n",
    "\n",
    "def measure_model_compute_time(model, obs_dim=7, BATCH=256, STEPS=1000):\n",
    "    policy = model.policy\n",
    "    x = th.randn(BATCH, obs_dim).to(policy.device)\n",
    "    lstm_state = policy.initial_state(batch_size=BATCH)\n",
    "    mask = th.ones(BATCH, 1).to(policy.device)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(STEPS):\n",
    "        policy.optimizer.zero_grad()\n",
    "        actions, lstm_state, _ = policy.forward(x, lstm_state, mask)\n",
    "        loss = actions.mean()  # dummy scalar\n",
    "        loss.backward()\n",
    "        policy.optimizer.step()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    print(f\"ML compute time: {t1 - t0:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML compute time: 5.691s\n"
     ]
    }
   ],
   "source": [
    "measure_model_compute_time(model1, obs_dim=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML compute time: 5.111s\n"
     ]
    }
   ],
   "source": [
    "measure_model_compute_time(model2, obs_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/20 for model1\n",
      "ML compute time: 5.155s\n",
      "Run 1/20 for model2\n",
      "ML compute time: 6.439s\n",
      "Run 2/20 for model1\n",
      "ML compute time: 5.090s\n",
      "Run 2/20 for model2\n",
      "ML compute time: 5.052s\n",
      "Run 3/20 for model1\n",
      "ML compute time: 5.350s\n",
      "Run 3/20 for model2\n",
      "ML compute time: 5.393s\n",
      "Run 4/20 for model1\n",
      "ML compute time: 5.672s\n",
      "Run 4/20 for model2\n",
      "ML compute time: 4.862s\n",
      "Run 5/20 for model1\n",
      "ML compute time: 5.288s\n",
      "Run 5/20 for model2\n",
      "ML compute time: 5.298s\n",
      "Run 6/20 for model1\n",
      "ML compute time: 5.434s\n",
      "Run 6/20 for model2\n",
      "ML compute time: 5.831s\n",
      "Run 7/20 for model1\n",
      "ML compute time: 6.980s\n",
      "Run 7/20 for model2\n",
      "ML compute time: 5.508s\n",
      "Run 8/20 for model1\n",
      "ML compute time: 5.662s\n",
      "Run 8/20 for model2\n",
      "ML compute time: 6.693s\n",
      "Run 9/20 for model1\n",
      "ML compute time: 5.458s\n",
      "Run 9/20 for model2\n",
      "ML compute time: 5.811s\n",
      "Run 10/20 for model1\n",
      "ML compute time: 5.502s\n",
      "Run 10/20 for model2\n",
      "ML compute time: 5.382s\n",
      "Run 11/20 for model1\n",
      "ML compute time: 5.224s\n",
      "Run 11/20 for model2\n",
      "ML compute time: 5.269s\n",
      "Run 12/20 for model1\n",
      "ML compute time: 5.381s\n",
      "Run 12/20 for model2\n",
      "ML compute time: 6.672s\n",
      "Run 13/20 for model1\n",
      "ML compute time: 5.275s\n",
      "Run 13/20 for model2\n",
      "ML compute time: 5.535s\n",
      "Run 14/20 for model1\n",
      "ML compute time: 5.809s\n",
      "Run 14/20 for model2\n",
      "ML compute time: 5.710s\n",
      "Run 15/20 for model1\n",
      "ML compute time: 6.504s\n",
      "Run 15/20 for model2\n",
      "ML compute time: 5.836s\n",
      "Run 16/20 for model1\n",
      "ML compute time: 6.012s\n",
      "Run 16/20 for model2\n",
      "ML compute time: 5.356s\n",
      "Run 17/20 for model1\n",
      "ML compute time: 5.854s\n",
      "Run 17/20 for model2\n",
      "ML compute time: 6.789s\n",
      "Run 18/20 for model1\n",
      "ML compute time: 5.297s\n",
      "Run 18/20 for model2\n",
      "ML compute time: 5.218s\n",
      "Run 19/20 for model1\n",
      "ML compute time: 5.794s\n",
      "Run 19/20 for model2\n",
      "ML compute time: 5.322s\n",
      "Run 20/20 for model1\n",
      "ML compute time: 5.986s\n",
      "Run 20/20 for model2\n",
      "ML compute time: 5.376s\n",
      "\n",
      "=== Benchmark results ===\n",
      "model1: mean=5.6363s, stdev=0.4720s\n",
      "model2: mean=5.6676s, stdev=0.5629s\n"
     ]
    }
   ],
   "source": [
    "import statistics as stats\n",
    "\n",
    "N_RUNS = 20  # how many times to repeat each measurement\n",
    "\n",
    "times_model1 = []\n",
    "times_model2 = []\n",
    "\n",
    "for i in range(N_RUNS):\n",
    "    print(f\"Run {i+1}/{N_RUNS} for model1\")\n",
    "    t1 = measure_model_compute_time(model1, obs_dim=7)  # assumes it returns time\n",
    "    times_model1.append(t1)\n",
    "\n",
    "    print(f\"Run {i+1}/{N_RUNS} for model2\")\n",
    "    t2 = measure_model_compute_time(model2, obs_dim=3)\n",
    "    times_model2.append(t2)\n",
    "\n",
    "print(\"\\n=== Benchmark results ===\")\n",
    "print(f\"model1: mean={stats.mean(times_model1):.4f}s, \"\n",
    "      f\"stdev={stats.stdev(times_model1):.4f}s\")\n",
    "print(f\"model2: mean={stats.mean(times_model2):.4f}s, \"\n",
    "      f\"stdev={stats.stdev(times_model2):.4f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainloop_py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
